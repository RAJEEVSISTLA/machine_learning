{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b6f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Equation\n",
      "0  C/O = 3/2\\n C = 50\\n 50/O = 3/2\\n 100 = 3O\\n O...\n",
      "1  2. C/O = 3/2\\n C = 60\\n 60/O = 3/2\\n 120 = 2O\\...\n",
      "2  3. C/O = 3/2\\n C = 60\\n 60/O = 3/2\\n 120 = 4O\\...\n",
      "3  4. C/O = 3/2\\n C = 70\\n 70/O = 3/2\\n 140 = 3O\\...\n",
      "4  5. C/O = 3/2\\n C = 60\\n 60/O = 3/2\\n 120 = 5O\\...\n",
      "                                            Equation  embedding_1  \\\n",
      "0  C/O = 3/2\\n C = 50\\n 50/O = 3/2\\n 100 = 3O\\n O...     0.030083   \n",
      "1  2. C/O = 3/2\\n C = 60\\n 60/O = 3/2\\n 120 = 2O\\...     0.299633   \n",
      "2  3. C/O = 3/2\\n C = 60\\n 60/O = 3/2\\n 120 = 4O\\...     0.339740   \n",
      "3  4. C/O = 3/2\\n C = 70\\n 70/O = 3/2\\n 140 = 3O\\...     0.281296   \n",
      "4  5. C/O = 3/2\\n C = 60\\n 60/O = 3/2\\n 120 = 5O\\...     0.297376   \n",
      "\n",
      "   embedding_2  embedding_3  embedding_4  embedding_5  embedding_6  \\\n",
      "0    -0.051380     0.595111    -0.092150     0.102375    -0.115757   \n",
      "1    -0.176748     0.667872    -0.006421     0.106535    -0.224786   \n",
      "2    -0.096238     0.754441    -0.024798     0.051905    -0.233466   \n",
      "3     0.072431     0.838212    -0.171771     0.158142    -0.253417   \n",
      "4    -0.058983     0.733095     0.023775     0.035412    -0.291897   \n",
      "\n",
      "   embedding_7  embedding_8  embedding_9  ...  embedding_759  embedding_760  \\\n",
      "0     0.116929    -0.325468     0.097051  ...       0.127265       0.138349   \n",
      "1    -0.002164    -0.234026     0.173633  ...       0.301112       0.135284   \n",
      "2     0.083101    -0.192481     0.102080  ...       0.165823       0.096068   \n",
      "3     0.126224    -0.103045     0.061743  ...       0.193062       0.141013   \n",
      "4     0.047481    -0.188632     0.079696  ...       0.188428       0.104485   \n",
      "\n",
      "   embedding_761  embedding_762  embedding_763  embedding_764  embedding_765  \\\n",
      "0       0.116750      -0.255835      -0.108881       0.067909      -0.441549   \n",
      "1      -0.023188      -0.224224      -0.037811      -0.027243      -0.204509   \n",
      "2       0.013411      -0.231135      -0.136883      -0.120354      -0.286222   \n",
      "3       0.067500      -0.394826      -0.153592      -0.009632      -0.219251   \n",
      "4       0.002517      -0.221798      -0.074107      -0.138417      -0.387409   \n",
      "\n",
      "   embedding_766  embedding_767  embedding_768  \n",
      "0      -0.325583       0.440513       0.122617  \n",
      "1      -0.305469       0.316496       0.215633  \n",
      "2      -0.395888       0.371992       0.193897  \n",
      "3      -0.304062       0.344096       0.233292  \n",
      "4      -0.344912       0.319690       0.125320  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "Embeddings have been saved to C:\\Users\\SISTLA RAHUL\\Desktop\\equation_with_embeddings.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Check if a GPU is available and use it if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file_path = r'C:\\Users\\SISTLA RAHUL\\Desktop\\MLPROJECT.xlsx'  # Update with your actual path\n",
    "data = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Check the first few rows of the data\n",
    "print(data.head())\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=False)\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Function to get BERT embeddings in batches\n",
    "def get_bert_embeddings_batch(text_list, batch_size=8):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process data in batches for efficiency\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the input texts in a batch\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Move inputs to the appropriate device (GPU or CPU)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # Get the BERT embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Take the mean across all tokens for each text\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().tolist()  # Move to CPU and convert to list\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Apply the function to the 'Equation' column in the dataset\n",
    "equations = data['Equation'].astype(str).tolist()\n",
    "embeddings = get_bert_embeddings_batch(equations, batch_size=16)  # Increase batch size for faster processing\n",
    "\n",
    "# Convert the list of embeddings into a DataFrame (each embedding element goes into its own column)\n",
    "embeddings_df = pd.DataFrame(embeddings, columns=[f'embedding_{i+1}' for i in range(len(embeddings[0]))])\n",
    "\n",
    "# Concatenate the original data with the embeddings DataFrame\n",
    "data_with_embeddings = pd.concat([data, embeddings_df], axis=1)\n",
    "\n",
    "# Save the embeddings to a new Excel file\n",
    "output_file_path = r'C:\\Users\\SISTLA RAHUL\\Desktop\\equation_with_embeddings.xlsx'  # Update with your output path\n",
    "data_with_embeddings.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Check the first few rows with embeddings\n",
    "print(data_with_embeddings.head())\n",
    "\n",
    "print(f\"Embeddings have been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cefae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
